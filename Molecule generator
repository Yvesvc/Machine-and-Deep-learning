'''######################
Dependencies
######################'''

import numpy as np
import rdkit.Chem as Chem
from rdkit.Chem import Draw
import matplotlib.pyplot as plt
import keras.backend as K
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
from keras.layers import Input, Dense, Lambda, TimeDistributed, Flatten, RepeatVector, Reshape
from keras.models import Model
from keras.objectives import binary_crossentropy
from keras.callbacks import LearningRateScheduler
from keras.optimizers import Adam

'''######################
Visualization of Molecule
######################'''

'''
#Visualize (Smile notated) String. eg draw_molecule_from_smile_string('CC1(C(N2C(S1)C(C2=O)NC(=O)CC3=CC=CC=C3)C(=O)O)C')
#Input: String
#Output: Matplotlib figure
'''
def draw_molecule_from_smile_string(smile_string):
    molecule_mol = Chem.MolFromSmiles(smile_string)
    return Draw.MolToMPL(molecule_mol, (200,200))
'''
#Visualize Smile Molecule from Smile file
#Input: smile_path: Path to file, row: n-th molecule in file
#Output: Matplotlib figure
'''
def draw_molecule_from_smile_file(smile_path,row):
	suppl = Chem.SmilesMolSupplier(smile_path,delimiter='\t',titleLine=False)
    return Draw.MolToMPL(suppl[row], (200,200))

'''######################
Data preparation
######################'''

'''
#Convert Smile encoded Molecules from txt file to one-hot-encoded array with shape (Number of samples, length largest molecule, number of possible characters)
Eg the first character of the second molecule is one-hot-encoded in position (1,2) with the character encoded as 1 in position that corresponds to that character.
#Input: path_smile_file: Path of the Smile file, nr_samples: number of samples in Smile file to convert
#Output: Numpy array with shape (Number of samples, length largest molecule, number of possible characters)
'''

def preprocess_smile(path_smile_file, nr_samples):
    #Store molecules in list of strings
    samples_string = open(path_smile_file, 'r').readlines()
    samples_string = [molecule[:-1] for molecule in samples_string] #Remove \n at end of each string

    #Convert list of strings to list of lists (eg list ['abc', 'def'] becomes [['a', 'b', 'c'], ['d','e', 'f']])
    samples_list = []
    for sample in samples_string:
        samples_list.append(list(sample))

    #Get all unique characters in list of strings and put in char:int dictionary
    samples_unique_char = set(''.join(samples_string))
    dict_unique_char = {}
    for nr , value in enumerate(samples_unique_char):
        dict_unique_char.update({value: nr})

    #Create reversed dictionary: int:char
    dict_unique_int = {int:char for char,int in dict_unique_char.items()}   

    #Convert characters in list of lists into integers using char:int dictionary
    samples_int = []
    for sample in samples_list:
        temp_list = []
        for char in sample:
            temp_list.extend([dict_unique_char.get(char)])
        samples_int.append(temp_list)

    #Get max length of Smile molecules, length of dictionary
    #pad each molecule list to max length of Smile molecules with value that is not in dictionary (to ensure uniqueness)
	#Padding is done because Neural network expects every input to have same shape
    max_length = len(max(samples_int,key=len))
    int_pad = len(dict_unique_char)
    for sample in samples_int:
        sample.extend([int_pad] * (max_length -len(sample)))

    #Convert list to numpy array
    samples_int = np.asarray(samples_int) #shape (number of molecules, number of unique characters)

    #get sub dataset from samples_int.
    sample_data_int = samples_int[:nr_samples]

    # the 3d array that will be the one-hot representation
    # a.max() + 1 is the number of labels we have
    training_data_oh = np.zeros((sample_data_int.shape[0], sample_data_int.shape[1], sample_data_int.max() + 1))

    # this first index selects each layer separately
    layer_idx = np.arange(sample_data_int.shape[0]).reshape(sample_data_int.shape[0], 1)

    # this index selects each component separately
    component_idx = np.tile(np.arange(sample_data_int.shape[1]), (sample_data_int.shape[0], 1))

    # then we use `a` to select indices according to category label
    training_data_oh[layer_idx, component_idx, sample_data_int] = 1
    
    return dict_unique_char, dict_unique_int, training_data_oh

'''######################
Data
######################'''

X_all = training_data_oh #shape (nr_samples, max_length, nr_unique_chars)
X_10000 = X_all[:10000,:,:] #shape (10000, max_length, nr_unique_chars)
X_0 = X_all[0].reshape(1,100,39) #shape (1, max_length, nr_unique_chars)

'''######################
Parameters
######################'''

#max_length
timesteps = X_all.shape[1]

#nr_unique_chars
n_features = X_all.shape[2]

#batch size
batch_size = 50

#optimizer
adam = Adam(lr=0.0015, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)

#size of latent space
n_z = 100

#number of epochs
n_epoch = 1

#Number of units in 1st hidden layer
h_1 = 200

#Number of units in 2nd hidden layer
h_2 = 40

#Relative importance of CCE vs KL in loss function
weight_cce = 30

'''######################
Architecture of Variational Autoencoder
######################'''

#INPUT#

#input layer shape for training is shape (max_length,nr_unique_chars) because takes one-hot-encoded molecules as input eg X_all
inputs = Input(shape=(timesteps,n_features), name = 'Input')

#ENCODER#

#hidden Dense layer 1
h_q = Dense(h_1, activation='tanh', name = 'enc_1')(inputs)

#hidden Dense layer 2
h_q_2 = Dense(h_2, activation='tanh', name = 'enc_2')(h_q)

#Flatten to convert matrix with shape (timesteps,n_features) to (timesteps*n_features) as latent space will be 1 dimensional
flat = Flatten()(h_q_2)

#mean with shape (n_z)
mu = Dense(n_z, activation='linear', name = 'mean')(flat)

#mu with shape (1,timesteps*n_features). Used as input for metrics function, not for training of network.
mu_repeat = RepeatVector(timesteps*n_features, name = 'mu_repeat')(tf.expand_dims(tf.reduce_mean(mu, axis=-1), axis=1))

#sddev vector with shape (n_z)
log_sigma = Dense(n_z, activation='linear', name = 'sddev')(flat)

#Sigma (not log_sigma) with shape (1,timesteps*n_features) Used as input for metrics function, not for training of network.
sigma_repeat = RepeatVector(timesteps*n_features, name = 'sigma_repeat')(tf.expand_dims(tf.reduce_mean(tf.math.exp(log_sigma), axis=-1), axis=1))

#function that returns z with shape (n_z) sampled from normal distribution with mean = mu and sddev = log_sigma
def sample_z(args):
    mu, log_sigma = args
    eps = K.random_normal(shape=(1, n_z), mean=0., stddev=1.)
    return mu + K.exp(log_sigma / 2) * eps

#Latent space layer with shape (batch_size,n_z) sampled from mean and sddev
z = Lambda(sample_z, name = 'latentspace')([mu, log_sigma])

#DECODER#

dense_z = Dense(timesteps*h_2, activation='tanh')
dense_dec = dense_z(z)
resh = Reshape((timesteps,h_2))
resh_dec= resh(dense_dec)
h_p = Dense(h_2, activation = 'tanh', name = 'dec_1')
h_p_dec = h_p(resh_dec)
h_p_2 = Dense(h_1, activation = 'tanh', name = 'dec_2')
h_p_2_dec = h_p_2(h_p_dec)
outputs = TimeDistributed(Dense(n_features, activation = 'softmax'), name = 'enc_output')
outputs_dec = outputs(h_p_2_dec)

#Layers used for sample generation. Input layer is shape(n_z) because takes output of Encoder = latent_space = z as input
input_z = Input(shape=(n_z,), name = 'Input_z')
dense_gen = dense_z(input_z) #
resh_gen= resh(dense_gen) 
h_p_gen = h_p(resh_gen) 
h_p_2_gen = h_p_2(h_p_gen) 
outputs_gen = outputs(h_p_2_gen)

'''######################
Loss functions (Code of available loss functions (https://github.com/keras-team/keras/blob/master/keras/losses.py) was used as a base to write custom loss function)
######################'''

'''
#categorical cross entropy loss for one one-hot encoded output to predict correct output
#Input: y_true and y_pred with shape (batch_size,timesteps,n_features). y_true/pred are mandatory arguments
#Ouput: Scalar = 1d tensor
'''
def cce_loss(y_true,y_pred):
    return - tf.reduce_sum(tf.reduce_sum(y_true * tf.log(y_pred), axis= -1), axis= -1) #return is scalar
'''
#categorical cross entropy loss for one one-hot encoded output to predict correct output + KL divergence to push mu towards 0 and sigma towards 1
#shape y_true/pred is (batch_size,timesteps,n_features)
#A detailed explanation of KL divergence and how it is vital for sample generation, can be found at : https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf
#Input: y_true and y_pred with shape (batch_size,timesteps,n_features)
#Ouput: Scalar = 1d tensor
'''
def total_loss(y_true,y_pred):
    cce = weight_cce * (- tf.reduce_sum(tf.reduce_sum(y_true * tf.log(y_pred), axis= -1), axis= -1)) #return is scalar
    kl = tf.reduce_sum((tf.math.exp(log_sigma) + tf.math.square(mu) - log_sigma - 1), axis = -1)
    return cce + kl	

'''######################
Metric functions (Code of available metric functions (https://github.com/keras-team/keras/blob/master/keras/metrics.py) was used as a base to write custom metric function)
######################'''

'''
#Returns average of mu.
#Input: y_true and y_pred with shape (batch_size,timesteps,n_features). y_true/pred are mandatory arguments
#Output: Tensor with shape (None,100,39)This shape because returned value needs to be of same shape as y_true/pred
'''
def metric_mu(y_true,y_pred):
    mu_reshaped = tf.reshape(mu_repeat, [batch_size,timesteps, n_features])
    return mu_reshaped

'''
#Returns average of sigma.
#Input: y_true and y_pred with shape (batch_size,timesteps,n_features). y_true/pred are mandatory arguments
#Output: Tensor with shape (None,100,39)This shape because returned value needs to be of same shape as y_true/pred
'''
def sigma_mu(y_true,y_pred):
    sigma_reshaped = tf.reshape(sigma_repeat, [batch_size,timesteps, n_features])
    return sigma_reshaped
	
'''######################
Model
######################'''
	
#Overall VAE model: encoder + decoder, for training#
vae = Model(inputs, outputs_dec)
vae.compile(optimizer= adam, loss=total_loss, metrics = ['categorical_accuracy', metric_mu, sigma_mu])
vae.summary()
vae.fit(X_all, X_all, batch_size=batch_size, epochs=n_epoch)

'''######################
Sample generation
######################'''

'''
#Given a sample, generate a new molecule. The similarity between both molecules can be altered with variability_factor
#Input: array with shape (1,timesteps,n_features)
#Output: array of shape(1,timesteps)
'''
def generate_from_sample(sample, variability_factor = 1):
	enc_mu = Model(inputs,mu) #Encoder to output mu
	enc_logsigma = Model(inputs,log_sigma) #Encoder to output log_sigma
	mu_sample = enc_mu.predict(sample) #mu
    logsigma_sample = enc_logsigma.predict(sample) #log_sigma
    #return z sampled from normal distribution with mean = mu and sddev = log_sigma. 
	#A higher variability factor results in a molecule that is less similar to provided sample. Because z is sampled from a distribution with increased sddev.
	eps = np.random.normal(loc=0., scale=1., size=n_z)
    z_gen = mu_sample + np.exp(logsigma_sample/2) * variability_factor * eps 
    generated = dec.predict(z_gen) #generate new molecule with shape (1,timesteps,n_features)
    generated = np.argmax(generated,axis=2) #Convert one-hot-encoding in 2nd dimension to number of corresponding character -> shape (1,timesteps)
    return generated

'''
#Convert generated molecule to string (with padded elements removed)
#Input: array with shape (1,timesteps)
#Output: String
'''	
def generated_to_int(generated_char):
    generated_int = []
    for element in generated_char[0]:
        if element != 38:
            generated_int.append(dict_unique_int.get(element))
        else: break
    return generated_int	

'''
#Generate and visualize a new molecule from a given sample molecule
#Input: Numpy array with shape (1,timesteps,n_features)
#Output: Matplotlib figure
'''
def molecule_generator(sample, variability_factor = 1):
    generated = generate_from_sample(sample, variability_factor)
    generated_int = generated_to_int(generated)
    generated_string = ''.join(generated_int)
    return draw_molecule_from_smile_string(generated_string)
	
	
	
